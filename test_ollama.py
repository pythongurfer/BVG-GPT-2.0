print("--- Iniciando prueba de conexiÃ³n con Ollama ---")

try:
    from langchain_community.llms import Ollama
    from langchain_community.embeddings import OllamaEmbeddings

    # --- Prueba 1: Generar un embedding ---
    print("\n[Paso 1/2] Probando el modelo de embeddings (nomic-embed-text)...")
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    test_vector = embeddings.embed_query("Esta es una prueba.")

    # Comprueba si el vector es vÃ¡lido
    if test_vector and isinstance(test_vector, list) and len(test_vector) > 0:
        print("âœ… Ã‰XITO: El embedding se generÃ³ correctamente.")
        print(f"   (Dimensiones del vector: {len(test_vector)})")
    else:
        print("âŒ FALLO: No se recibiÃ³ un embedding vÃ¡lido.")

    # --- Prueba 2: Generar una respuesta de texto ---
    print("\n[Paso 2/2] Probando el modelo de lenguaje (llama3:8b)...")
    llm = Ollama(model="llama3:8b")
    response = llm.invoke("Di 'hola' en espaÃ±ol.")

    if response and isinstance(response, str) and len(response) > 0:
        print("âœ… Ã‰XITO: El LLM generÃ³ una respuesta.")
        print(f"   Respuesta del modelo: '{response.strip()}'")
    else:
        print("âŒ FALLO: No se recibiÃ³ una respuesta vÃ¡lida del LLM.")

except Exception as e:
    print("\nğŸš¨ Â¡ERROR CRÃTICO DURANTE LA PRUEBA!")
    print(f"   El error es: {e}")

print("\n--- Fin de la prueba ---")